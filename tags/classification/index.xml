<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Classification on Wenlong Yi (Evan)</title><link>https://yiwenlong2001.github.io/tags/classification/</link><description>Recent content in Classification on Wenlong Yi (Evan)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 30 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yiwenlong2001.github.io/tags/classification/index.xml" rel="self" type="application/rss+xml"/><item><title>Classification review notes (updating...)</title><link>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</guid><description>&lt;img src="https://yiwenlong2001.github.io/p/classification-review-notes-updating.../classification.png" alt="Featured image of post Classification review notes (updating...)" />&lt;h1 id="classification">Classification&lt;/h1>
&lt;h2 id="decision-tree">Decision Tree&lt;/h2>
&lt;ul>
&lt;li>决策树最主要的就是构建决策树，构建决策树有以下几种著名的算法。&lt;/li>
&lt;/ul>
&lt;h3 id="id3算法">ID3算法&lt;/h3>
&lt;p>ID3算法是一种基于信息熵的算法。通过信息增益找到可以最合适的划分属性。&lt;/p>
&lt;blockquote>
&lt;p>Gain(S, A) = E(S) - E(S|A)&lt;br>
E(S|A) = - sum(|S_i|/|S| * E(S_1))&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>计算所有的属性对应的信息增益，选择信息增益最大的属性对数据集进行划分。&lt;/li>
&lt;li>递归地对之后的数据集进行划分直到：
&lt;ul>
&lt;li>只剩一个属性无法继续划分&lt;/li>
&lt;li>所有的属性信息增益都很小&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>缺点：&lt;/p>
&lt;ul>
&lt;li>无剪枝策略，容易过拟合；&lt;/li>
&lt;li>只能用于处理离散分布的特征；&lt;/li>
&lt;li>没有考虑缺失值。&lt;/li>
&lt;li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1，而信息增益比指数可以解决此缺点。&lt;/li>
&lt;/ul>
&lt;p>为了解决这些问题，提出这些改进：&lt;/p>
&lt;h3 id="gain-ratio">Gain ratio&lt;/h3>
&lt;p>$ SplitInformation = \sum_{i=1}^{c} \frac{|S_i|}{|S|} log_2 \frac{|S_i|}{|S|} $
$ GainRatio(S, A) = \frac{Gain(S, A)}{SplitInformatio(S, A)} $&lt;/p>
&lt;h3 id="gini-index">Gini Index&lt;/h3>
&lt;p>如果一个数据集有n各种类的数据，那么gini index被定义为
$$ gini(T) = 1 - \sum_{j=1}^n p_j^2$$
其中$ p_j $是种类j再数据集中的频繁程度&lt;/p>
&lt;p>假如我们将数据集T分为大小分别为N1和N2的两个子集T1和T2， 那么此次分割中：
$$ gini_{split}(T) = \frac{N_1}{N} gini(T_1) + \frac{N_2}{N} gini(T_2) $$&lt;/p>
&lt;p>在各种分割中，选择$ gini_{split}(T) $最小的分法。&lt;/p>
&lt;h3 id="过拟合">过拟合&lt;/h3>
&lt;p>针对过拟合可以采用以下两种方法：&lt;/p>
&lt;ul>
&lt;li>Prepruning: Halt tree construction early—do not split a node if this would result in the goodness measure falling below a threshold&lt;/li>
&lt;li>Postpruning: Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees&lt;/li>
&lt;/ul>
&lt;p>确定树的大小：使用MDL原则&lt;/p>
&lt;h2 id="贝叶斯网络">贝叶斯网络&lt;/h2>
&lt;p>贝叶斯定理：&lt;br>
given training data x, posteriori probability of a hypothesis H, P(H|X) is $ P(H|X) = \frac{P(X|H) * P(H)}{P(X)} $&lt;/p>
&lt;ul>
&lt;li>P(H|X) Posterior probability is the updates probability after the evidence is considered.&lt;/li>
&lt;li>P(X|H) Likelihood probability&lt;/li>
&lt;/ul>
&lt;h3 id="naive-bayesian-classifier">naive bayesian classifier&lt;/h3>
&lt;p>&amp;ldquo;naive&amp;rdquo; means it assumes class conditional independence.&lt;/p>
&lt;p>Example:&lt;br>
P(buy = yes | age &amp;lt;= 30, income = high, student = no).&lt;br>
Apply Bayes&amp;rsquo;s. theorem:&lt;/p>
&lt;ul>
&lt;li>P(B=Y|A&amp;lt;=30, I=H, S=N) = P(B=Y) * P(A&amp;lt;=30, I=H, S=N | B=Y) / P(A&amp;lt;=30, I=H, S=N)&lt;/li>
&lt;li>其中P(B=Y)直接从数据中得到&lt;/li>
&lt;li>P(A&amp;lt;=30, I=H, S=N | B=Y)由于独立性可以分解为：P(A&amp;lt;=30 | B=Y) * P(I=H | B=Y) * P(S=N | B=Y)，这三个可以直接从数据中count得到&lt;/li>
&lt;/ul>
&lt;h3 id="bayesian-network">Bayesian Network&lt;/h3>
&lt;p>No longer independent!&lt;br>
consider: Causal relation (depending relationship) instead.&lt;/p>
&lt;p>Conditional Independent:&lt;/p>
&lt;ul>
&lt;li>S and R is not independent.&lt;/li>
&lt;li>S given C and R given C is independent!&lt;/li>
&lt;/ul>
&lt;p>for a struction like that:&lt;br>
C&lt;br>
S R&lt;br>
W&lt;br>
we can construct P(C, S, R, W) = P(W|S, R) * P(R|C) * P(S|C) * P(C)&lt;/p>
&lt;p>Inference: Bottom up.&lt;br>
First we calculate P(W=T)
$$ \sum_{c,s,r} P(C=c, S=s, R=r, W=T)$$
and for two possible reason S and R
P(S|R) = P(S, R) / P(R)&lt;/p>
&lt;p>Inference: Top down
The probability that W=T given that C = T&lt;/p></description></item></channel></rss>