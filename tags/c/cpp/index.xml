<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>C/CPP on Wenlong Yi (Evan)</title><link>https://yiwenlong2001.github.io/tags/c/cpp/</link><description>Recent content in C/CPP on Wenlong Yi (Evan)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://yiwenlong2001.github.io/tags/c/cpp/index.xml" rel="self" type="application/rss+xml"/><item><title>PyTorch CUDA Operators Parallelism Implementation and Optimization</title><link>https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/</link><pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate><guid>https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/</guid><description>&lt;img src="https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/GPU.jpg" alt="Featured image of post PyTorch CUDA Operators Parallelism Implementation and Optimization" />&lt;ul>
&lt;li>Employed warp-level primitives to optimize warp reduce of Softmax Kernel, which improved efficiency by 7%&lt;/li>
&lt;li>Implemented matrix multiplication kernel with matrix chunking and shared memory in C and CUDA&lt;/li>
&lt;li>Optimized shared memory access via vector read instruction LDS.128, resulting in a 3% efficiency improvement
&lt;a class="link" href="https://github.com/wangshanyw/PyTorch-CUDA-Operators-Implementation-and-Optimization" target="_blank" rel="noopener"
>&lt;span style="color:blue"> Github Repo Link &lt;span>&lt;/a>
&lt;a class="link" href="https://docs.google.com/presentation/d/1uhkq8XJ8SvoxHUiech5nn_noU7b2TYk7/edit?usp=sharing&amp;amp;ouid=108660935975018643927&amp;amp;rtpof=true&amp;amp;sd=true" target="_blank" rel="noopener"
>&lt;span style="color:blue"> Docs Link &lt;span>&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>