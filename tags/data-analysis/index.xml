<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data analysis on Wenlong Yi (Evan)</title><link>https://yiwenlong2001.github.io/tags/data-analysis/</link><description>Recent content in Data analysis on Wenlong Yi (Evan)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 30 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yiwenlong2001.github.io/tags/data-analysis/index.xml" rel="self" type="application/rss+xml"/><item><title>Classification review notes (updating...)</title><link>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</link><pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate><guid>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</guid><description>&lt;img src="https://yiwenlong2001.github.io/p/classification-review-notes-updating.../classification.png" alt="Featured image of post Classification review notes (updating...)" />&lt;h1 id="classification">Classification&lt;/h1>
&lt;h2 id="decision-tree">Decision Tree&lt;/h2>
&lt;ul>
&lt;li>决策树最主要的就是构建决策树，构建决策树有以下几种著名的算法。&lt;/li>
&lt;/ul>
&lt;h3 id="id3算法">ID3算法&lt;/h3>
&lt;p>ID3算法是一种基于信息熵的算法。通过信息增益找到可以最合适的划分属性。&lt;/p>
&lt;blockquote>
&lt;p>Gain(S, A) = E(S) - E(S|A)&lt;br>
E(S|A) = - sum(|S_i|/|S| * E(S_1))&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>计算所有的属性对应的信息增益，选择信息增益最大的属性对数据集进行划分。&lt;/li>
&lt;li>递归地对之后的数据集进行划分直到：
&lt;ul>
&lt;li>只剩一个属性无法继续划分&lt;/li>
&lt;li>所有的属性信息增益都很小&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>缺点：&lt;/p>
&lt;ul>
&lt;li>无剪枝策略，容易过拟合；&lt;/li>
&lt;li>只能用于处理离散分布的特征；&lt;/li>
&lt;li>没有考虑缺失值。&lt;/li>
&lt;li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1，而信息增益比指数可以解决此缺点。&lt;/li>
&lt;/ul>
&lt;p>为了解决这些问题，提出这些改进：&lt;/p>
&lt;h3 id="gain-ratio">Gain ratio&lt;/h3>
&lt;p>$ SplitInformation = \sum_{i=1}^{c} \frac{|S_i|}{|S|} log_2 \frac{|S_i|}{|S|} $
$ GainRatio(S, A) = \frac{Gain(S, A)}{SplitInformatio(S, A)} $&lt;/p>
&lt;h3 id="gini-index">Gini Index&lt;/h3>
&lt;p>如果一个数据集有n各种类的数据，那么gini index被定义为
$$ gini(T) = 1 - \sum_{j=1}^n p_j^2$$
其中$ p_j $是种类j再数据集中的频繁程度&lt;/p>
&lt;p>假如我们将数据集T分为大小分别为N1和N2的两个子集T1和T2， 那么此次分割中：
$$ gini_{split}(T) = \frac{N_1}{N} gini(T_1) + \frac{N_2}{N} gini(T_2) $$&lt;/p>
&lt;p>在各种分割中，选择$ gini_{split}(T) $最小的分法。&lt;/p>
&lt;h3 id="过拟合">过拟合&lt;/h3>
&lt;p>针对过拟合可以采用以下两种方法：&lt;/p>
&lt;ul>
&lt;li>Prepruning: Halt tree construction early—do not split a node if this would result in the goodness measure falling below a threshold&lt;/li>
&lt;li>Postpruning: Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees&lt;/li>
&lt;/ul>
&lt;p>确定树的大小：使用MDL原则&lt;/p>
&lt;h2 id="贝叶斯网络">贝叶斯网络&lt;/h2>
&lt;p>贝叶斯定理：&lt;br>
given training data x, posteriori probability of a hypothesis H, P(H|X) is $ P(H|X) = \frac{P(X|H) * P(H)}{P(X)} $&lt;/p>
&lt;ul>
&lt;li>P(H|X) Posterior probability is the updates probability after the evidence is considered.&lt;/li>
&lt;li>P(X|H) Likelihood probability&lt;/li>
&lt;/ul>
&lt;h3 id="naive-bayesian-classifier">naive bayesian classifier&lt;/h3>
&lt;p>&amp;ldquo;naive&amp;rdquo; means it assumes class conditional independence.&lt;/p>
&lt;p>Example:&lt;br>
P(buy = yes | age &amp;lt;= 30, income = high, student = no).&lt;br>
Apply Bayes&amp;rsquo;s. theorem:&lt;/p>
&lt;ul>
&lt;li>P(B=Y|A&amp;lt;=30, I=H, S=N) = P(B=Y) * P(A&amp;lt;=30, I=H, S=N | B=Y) / P(A&amp;lt;=30, I=H, S=N)&lt;/li>
&lt;li>其中P(B=Y)直接从数据中得到&lt;/li>
&lt;li>P(A&amp;lt;=30, I=H, S=N | B=Y)由于独立性可以分解为：P(A&amp;lt;=30 | B=Y) * P(I=H | B=Y) * P(S=N | B=Y)，这三个可以直接从数据中count得到&lt;/li>
&lt;/ul>
&lt;h3 id="bayesian-network">Bayesian Network&lt;/h3>
&lt;p>No longer independent!&lt;br>
consider: Causal relation (depending relationship) instead.&lt;/p>
&lt;p>Conditional Independent:&lt;/p>
&lt;ul>
&lt;li>S and R is not independent.&lt;/li>
&lt;li>S given C and R given C is independent!&lt;/li>
&lt;/ul>
&lt;p>for a struction like that:&lt;br>
C&lt;br>
S R&lt;br>
W&lt;br>
we can construct P(C, S, R, W) = P(W|S, R) * P(R|C) * P(S|C) * P(C)&lt;/p>
&lt;p>Inference: Bottom up.&lt;br>
First we calculate P(W=T)
$$ \sum_{c,s,r} P(C=c, S=s, R=r, W=T)$$
and for two possible reason S and R
P(S|R) = P(S, R) / P(R)&lt;/p>
&lt;p>Inference: Top down
The probability that W=T given that C = T&lt;/p></description></item><item><title>Cluster review notes (updating...)</title><link>https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../</guid><description>&lt;img src="https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../cluster.jpg" alt="Featured image of post Cluster review notes (updating...)" />&lt;h1 id="cluster">Cluster&lt;/h1>
&lt;h2 id="algorithm_1-k-means">Algorithm_1 K-means&lt;/h2>
&lt;ul>
&lt;li>randomly choose initial cluster centroics.&lt;/li>
&lt;li>assign each point to its nearset centroic.&lt;/li>
&lt;li>do iteration until the location of any centroics does not change:
&lt;ul>
&lt;li>caluculate the mean value for every point of each cluster.&lt;/li>
&lt;li>relocate every point.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="algorithm_2-pam">Algorithm_2 PAM&lt;/h2>
&lt;ul>
&lt;li>Arbitrarily choose k objects as the initial medoids&lt;/li>
&lt;li>Until no change, do
&lt;ul>
&lt;li>(Re)assign each object to the cluster to which the nearest medoid&lt;/li>
&lt;li>Randomly select a non-medoid object o’, compute the total cost, S, of swapping medoid o with o’&lt;/li>
&lt;li>If S &amp;lt; 0 then swap o with o’ to form the new set of k medoids&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Swapping Cost:&lt;/strong> Measure whether o’ is better than o as a medoid&lt;/p>
&lt;/blockquote>
&lt;h2 id="algorithm_3-clara">Algorithm_3 CLARA&lt;/h2>
&lt;ul>
&lt;li>random sample in huge data&lt;/li>
&lt;li>do PAM&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>PAM search the whole graph&lt;br>
CLARA search some random sub-graphs&lt;/p>
&lt;/blockquote>
&lt;h2 id="algorithm_4-hierarchical-clustering">Algorithm_4 Hierarchical Clustering&lt;/h2>
&lt;h3 id="agnes-agglomerative">AGNES (agglomerative)&lt;/h3>
&lt;ul>
&lt;li>Initially, each object is a cluster&lt;/li>
&lt;li>Step-by-step cluster merging, until all objects form a cluster&lt;/li>
&lt;/ul>
&lt;h3 id="diana-divisive">DIANA (divisive)&lt;/h3>
&lt;ul>
&lt;li>Initially, all objects are in one cluster&lt;/li>
&lt;li>Step-by-step splitting clusters until each cluster contains only one object&lt;/li>
&lt;/ul>
&lt;h3 id="more-details">More Details&lt;/h3>
&lt;ul>
&lt;li>Single-Link（最短距离链接）：&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Single-Link聚类方法将两个簇之间的距离定义为它们中最接近的两个点之间的距离。换句话说，它测量了两个簇中最相似的成员之间的距离。这种方法倾向于形成具有长而窄的簇，因为它强调了局部相似性。Single-Link在处理非凸形状的簇时表现良好，但容易受到噪声和异常值的影响。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Complete-Link（最长距离链接）：&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Complete-Link聚类方法将两个簇之间的距离定义为它们中最不相似的两个点之间的距离。它关注的是两个簇中最不相似的成员之间的距离。这种方法更倾向于形成具有更紧凑形状的簇，因为它强调了簇的全局相似性。Complete-Link对于处理不同大小和不同密度的簇效果较好，但可能会受到异常值的干扰。&lt;/p>
&lt;/blockquote>
&lt;h2 id="algorithm_5-birch">Algorithm_5 BIRCH&lt;/h2>
&lt;blockquote>
&lt;p>Clustering Feature: CF = (N, LS, SS)&lt;br>
N: #data points&lt;br>
LS: sum of position&lt;br>
SS: sum of the square of the position&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Phase 1: scan DB to build an initial inmemory CF tree (a multi-level compression of the data that tries to preserve the inherent clustering structure of the data)&lt;/li>
&lt;li>Phase 2: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../birch.png"
width="1066"
height="783"
srcset="https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../birch_hu1f7cfcf85d02ae0cac7b9002dcc28f68_55343_480x0_resize_box_3.png 480w, https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../birch_hu1f7cfcf85d02ae0cac7b9002dcc28f68_55343_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="326px"
>&lt;/p>
&lt;blockquote>
&lt;p>一开始只有一个空的root，然后一个一个加进去，如果在已有节点的球体内，则合为一个cluster，如果一个cluster超过一定数量，则分裂：选择最远两个点最为两个新的cluster中心点，重新分配，保证树结构完整性。&lt;/p>
&lt;/blockquote>
&lt;h2 id="algorithm_6-distance-based-methods">Algorithm_6 Distance-based Methods&lt;/h2>
&lt;h3 id="previous-knowledge">Previous Knowledge&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Eps&lt;/strong>: Maximum radius of the neighborhood&lt;/li>
&lt;li>&lt;strong>MinPts&lt;/strong>: Minimum number of points in an Eps- neighborhood of that point&lt;/li>
&lt;li>&lt;strong>NEps(p)&lt;/strong>: {q | dist(p,q) $\leq$ Eps}&lt;/li>
&lt;li>&lt;strong>Core object p&lt;/strong>: |Neps(p)| $\ge$ MinPts&lt;/li>
&lt;li>Point q &lt;strong>directly density-reachable&lt;/strong> from p iff q $\in$ Neps(p) and p is a core object&lt;/li>
&lt;li>&lt;strong>Density-reachable:&lt;/strong> p1 $\rightarrow$ p2, p2 $\rightarrow$ p3, &amp;hellip;, pn-1 $\rightarrow$ pn then pn is density-reachable from p1&lt;/li>
&lt;li>&lt;strong>Density-connected:&lt;/strong> Points p, q are density-reachable from o $\rightarrow$ p and q are density-connected&lt;/li>
&lt;/ul>
&lt;h3 id="dbscan">DBSCAN&lt;/h3>
&lt;ul>
&lt;li>Arbitrary select a point p&lt;/li>
&lt;li>Retrieve all points directly density-reachable from p wrt Eps and MinPts&lt;/li>
&lt;li>If p is a core point, a cluster is formed&lt;/li>
&lt;li>If p is a border point, no points are density- reachable from p and DBSCAN visits the next point of the database&lt;/li>
&lt;li>Continue the process until all of the points have been processed&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../DBSCAN.png"
width="2154"
height="820"
srcset="https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../DBSCAN_huabd1eadf2ef4cdb183dd88d1d60e4bc1_347203_480x0_resize_box_3.png 480w, https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../DBSCAN_huabd1eadf2ef4cdb183dd88d1d60e4bc1_347203_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="262"
data-flex-basis="630px"
>&lt;/p>
&lt;blockquote>
&lt;p>P.S. 一个一个判断，如果是cluster，且范围内点已形成cluster，则加入，若无先前cluster，则新建立一个。若是后建立的和之前建立的通过一个点reachable了，则将两个cluster合为一个。&lt;/p>
&lt;/blockquote></description></item></channel></rss>