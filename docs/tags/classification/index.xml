<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Classification on Wenlong Yi (Evan)</title>
        <link>https://yiwenlong2001.github.io/tags/classification/</link>
        <description>Recent content in Classification on Wenlong Yi (Evan)</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 30 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yiwenlong2001.github.io/tags/classification/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Classification review notes (updating...)</title>
        <link>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</link>
        <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
        
        <guid>https://yiwenlong2001.github.io/p/classification-review-notes-updating.../</guid>
        <description>&lt;img src="https://yiwenlong2001.github.io/p/classification-review-notes-updating.../classification.png" alt="Featured image of post Classification review notes (updating...)" /&gt;&lt;h1 id=&#34;classification&#34;&gt;Classification&lt;/h1&gt;
&lt;h2 id=&#34;decision-tree&#34;&gt;Decision Tree&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;决策树最主要的就是构建决策树，构建决策树有以下几种著名的算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;id3算法&#34;&gt;ID3算法&lt;/h3&gt;
&lt;p&gt;ID3算法是一种基于信息熵的算法。通过信息增益找到可以最合适的划分属性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gain(S, A) = E(S) - E(S|A)&lt;br&gt;
E(S|A) = - sum(|S_i|/|S| * E(S_1))&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;计算所有的属性对应的信息增益，选择信息增益最大的属性对数据集进行划分。&lt;/li&gt;
&lt;li&gt;递归地对之后的数据集进行划分直到：
&lt;ul&gt;
&lt;li&gt;只剩一个属性无法继续划分&lt;/li&gt;
&lt;li&gt;所有的属性信息增益都很小&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无剪枝策略，容易过拟合；&lt;/li&gt;
&lt;li&gt;只能用于处理离散分布的特征；&lt;/li&gt;
&lt;li&gt;没有考虑缺失值。&lt;/li&gt;
&lt;li&gt;信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1，而信息增益比指数可以解决此缺点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解决这些问题，提出这些改进：&lt;/p&gt;
&lt;h3 id=&#34;gain-ratio&#34;&gt;Gain ratio&lt;/h3&gt;
&lt;p&gt;$ SplitInformation = \sum_{i=1}^{c} \frac{|S_i|}{|S|} log_2 \frac{|S_i|}{|S|} $
$ GainRatio(S, A) = \frac{Gain(S, A)}{SplitInformatio(S, A)} $&lt;/p&gt;
&lt;h3 id=&#34;gini-index&#34;&gt;Gini Index&lt;/h3&gt;
&lt;p&gt;如果一个数据集有n各种类的数据，那么gini index被定义为
$$ gini(T) = 1 - \sum_{j=1}^n p_j^2$$
其中$ p_j $是种类j再数据集中的频繁程度&lt;/p&gt;
&lt;p&gt;假如我们将数据集T分为大小分别为N1和N2的两个子集T1和T2， 那么此次分割中：
$$ gini_{split}(T) = \frac{N_1}{N} gini(T_1) + \frac{N_2}{N} gini(T_2) $$&lt;/p&gt;
&lt;p&gt;在各种分割中，选择$ gini_{split}(T) $最小的分法。&lt;/p&gt;
&lt;h3 id=&#34;过拟合&#34;&gt;过拟合&lt;/h3&gt;
&lt;p&gt;针对过拟合可以采用以下两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepruning: Halt tree construction early—do not split a node if this would result in the goodness measure falling below a threshold&lt;/li&gt;
&lt;li&gt;Postpruning: Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;确定树的大小：使用MDL原则&lt;/p&gt;
&lt;h2 id=&#34;贝叶斯网络&#34;&gt;贝叶斯网络&lt;/h2&gt;
&lt;p&gt;贝叶斯定理：&lt;br&gt;
given training data x, posteriori probability of a hypothesis H, P(H|X) is $ P(H|X) = \frac{P(X|H) * P(H)}{P(X)} $&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(H|X) Posterior probability is the updates probability after the evidence is considered.&lt;/li&gt;
&lt;li&gt;P(X|H) Likelihood probability&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;naive-bayesian-classifier&#34;&gt;naive bayesian classifier&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;naive&amp;rdquo; means it assumes class conditional independence.&lt;/p&gt;
&lt;p&gt;Example:&lt;br&gt;
P(buy = yes | age &amp;lt;= 30, income = high, student = no).&lt;br&gt;
Apply Bayes&amp;rsquo;s. theorem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B=Y|A&amp;lt;=30, I=H, S=N) = P(B=Y) * P(A&amp;lt;=30, I=H, S=N | B=Y) / P(A&amp;lt;=30, I=H, S=N)&lt;/li&gt;
&lt;li&gt;其中P(B=Y)直接从数据中得到&lt;/li&gt;
&lt;li&gt;P(A&amp;lt;=30, I=H, S=N | B=Y)由于独立性可以分解为：P(A&amp;lt;=30 | B=Y) * P(I=H | B=Y) * P(S=N | B=Y)，这三个可以直接从数据中count得到&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-network&#34;&gt;Bayesian Network&lt;/h3&gt;
&lt;p&gt;No longer independent!&lt;br&gt;
consider: Causal relation (depending relationship) instead.&lt;/p&gt;
&lt;p&gt;Conditional Independent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S and R is not independent.&lt;/li&gt;
&lt;li&gt;S given C and R given C is independent!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;for a struction like that:&lt;br&gt;
C&lt;br&gt;
S  R&lt;br&gt;
W&lt;br&gt;
we can construct P(C, S, R, W) = P(W|S, R) * P(R|C) * P(S|C) * P(C)&lt;/p&gt;
&lt;p&gt;Inference: Bottom up.&lt;br&gt;
First we calculate P(W=T)
$$ \sum_{c,s,r} P(C=c, S=s, R=r, W=T)$$
and for two possible reason S and R
P(S|R) = P(S, R) / P(R)&lt;/p&gt;
&lt;p&gt;Inference: Top down
The probability that W=T given that C = T&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
