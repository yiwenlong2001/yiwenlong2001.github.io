[{"content":"Education Ungraduate School Shanghai Jiao Tong University Degree：Bachelor Major：Computer Science Date：2019-9-1 to 2023-6-30 Award：2019, 2020, 2021 SJTU Honor Award Award：Outstanding Graduates Relevant coursework Data Science, Artificial Intelligence, Data Structure, Algorithm, Database, Machine Learning, Computer Network, Software Engineer Graduate School University of California, Los Angeles Degree：Master Major：Computer Science Date：2023-9-1 to 2025-5-1 Relevant coursework Reinforcement Learning, Big Data Analysis, Automatic Reasoning Activity SJTU Deputy Director of the Social Practice Department of the Youth League Committee\nUCLA Waiting to be filled\n","date":"2023-09-01T00:00:00Z","image":"https://yiwenlong2001.github.io/p/education/ucla_hueb66c70f98b40f4c1056d1dc09470407_175613_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/education/","title":"Education"},{"content":"Classification Decision Tree 决策树最主要的就是构建决策树，构建决策树有以下几种著名的算法。 ID3算法 ID3算法是一种基于信息熵的算法。通过信息增益找到可以最合适的划分属性。\nGain(S, A) = E(S) - E(S|A)\nE(S|A) = - sum(|S_i|/|S| * E(S_1))\n计算所有的属性对应的信息增益，选择信息增益最大的属性对数据集进行划分。 递归地对之后的数据集进行划分直到： 只剩一个属性无法继续划分 所有的属性信息增益都很小 缺点：\n无剪枝策略，容易过拟合； 只能用于处理离散分布的特征； 没有考虑缺失值。 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1，而信息增益比指数可以解决此缺点。 为了解决这些问题，提出这些改进：\nGain ratio $ SplitInformation = \\sum_{i=1}^{c} \\frac{|S_i|}{|S|} log_2 \\frac{|S_i|}{|S|} $ $ GainRatio(S, A) = \\frac{Gain(S, A)}{SplitInformatio(S, A)} $\nGini Index 如果一个数据集有n各种类的数据，那么gini index被定义为 $$ gini(T) = 1 - \\sum_{j=1}^n p_j^2$$ 其中$ p_j $是种类j再数据集中的频繁程度\n假如我们将数据集T分为大小分别为N1和N2的两个子集T1和T2， 那么此次分割中： $$ gini_{split}(T) = \\frac{N_1}{N} gini(T_1) + \\frac{N_2}{N} gini(T_2) $$\n在各种分割中，选择$ gini_{split}(T) $最小的分法。\n过拟合 针对过拟合可以采用以下两种方法：\nPrepruning: Halt tree construction early—do not split a node if this would result in the goodness measure falling below a threshold Postpruning: Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees 确定树的大小：使用MDL原则\n贝叶斯网络 ","date":"2023-10-30T00:00:00Z","image":"https://yiwenlong2001.github.io/p/classification-review-notes-updating.../classification_hu4fc43e2b18ace3d44063bbef11699032_4250_120x120_fill_box_smart1_3.png","permalink":"https://yiwenlong2001.github.io/p/classification-review-notes-updating.../","title":"Classification review notes (updating...)"},{"content":"NNF 和 对应拓展性质 NNF 输入为variable以及variable的否定，由与或门组成的逻辑网络 这是一种最常见的逻辑网络，我们没办法做什么 DNNF 在NNF的基础上，要求每一个and节点的任意两个分支所涉及的节点交集为空（no overlap） 在这个网络上，NNF的可行性可以被简单地验证 我们可以在多项式时间内执行：CO, CE, ME这三个任务 任务汇总：\nCO: consistency\nVA: Validity\nSE: sentential entailment\nCE: clausal entailment(KB implies clause)\nIP: implicant testing(term implies KB)\nEQ: equivalence testing\nCT: model countig\nME: model enumeration\nDNNF能够在线性时间内完成clause entailment 如果一个NNF子图能够在线性时间内完成clause entailment，我们称之为tractable。DNNF是tractable的。\ndNNF 在NNF的基础上，要求每一个or节点的任意两个分支的输入之间互斥 E.g. $(\\neg A \\land B) \\lor (A \\land \\neg B)$ 就是互斥的\n这意味着无论你怎么取值，or节点的不可能有两个输入同时为true dNNF不是tractable的 sNNF \u0026ldquo;smoothness\u0026rdquo;，在NNF的基础上，要求每一个or节点的任意两个分支所涉及的参数必须相同。 sNNF不是tractable的。 d-DNNF 顾名思义，同时满足dNNF和DNNF。 d-DNNF是tractable的。 d-DNNF可以在多项式时间内实现VA， IP， CT任务。 sd-DNNF 同时满足dNNF和DNNF和sNNF tractable。 f-NNF flatness or shallow circuit 满足性质：网络仅仅只有两层 Simple conjunction：由or-and-variable组成，且每个and下的variable均各不相同，我们可以将这种网络改写为DNF形式。 Simple conjunction同时满足decomposity性质。 Simple disjunction，CNF，同上。 f-NNF和CNF不是tractable。但是DNF是tractable的。 PI prime Implicates, 是一种特殊的CNF，要求CNF满足： 没有一个clause是其他clause的子集（都是有用的clause） 如果可以进行resolution，那么resolution的结果一定本来就可以被其中某个clause imply。 DNF的版本称之为IP PI来自于CNF， 可以在多项式时间内解决CO, CE, ME, VA, IP, SE, EQ问题 IP来自于DNF，可以额外在多项式时间内解决VA, IP, SE, EQ问题。 BDD decision graph： variable的取值由其parent决定。 同时还是decomposition。 FBDD test one property：从任何一条根节点到叶子节点的路径上，每一个variable都只能出现一次 是在d-DNNF和BDD的基础上的，因此他可以在多项式时间内解决七个问题。 OBDD 在任意从根节点到叶子节点的路径上，都满足同一variable的顺序 decision decomposability order 可以在多项式时间内完成SE， EQ任务，因此它在多项式时间内可以完成所有的任务。 DNNF详解 transformation operation:\nCD: conditioning.\nSFO: single variable.\nFO: multiple variable.\n\u0026amp;: conjunction.\nB\u0026amp;: bounded conjoin.\n|: disjoin.\nB|: bounded disjoin.\n~: negate\n","date":"2023-10-28T00:00:00Z","image":"https://yiwenlong2001.github.io/p/nnf-review-notes-updating.../NNF_hu14edb053461a8e06e4f1fdca463dbb61_2574_120x120_fill_box_smart1_3.png","permalink":"https://yiwenlong2001.github.io/p/nnf-review-notes-updating.../","title":"NNF review notes (updating...)"},{"content":"Cluster Algorithm_1 K-means randomly choose initial cluster centroics. assign each point to its nearset centroic. do iteration until the location of any centroics does not change: caluculate the mean value for every point of each cluster. relocate every point. Algorithm_2 PAM Arbitrarily choose k objects as the initial medoids Until no change, do (Re)assign each object to the cluster to which the nearest medoid Randomly select a non-medoid object o’, compute the total cost, S, of swapping medoid o with o’ If S \u0026lt; 0 then swap o with o’ to form the new set of k medoids Swapping Cost: Measure whether o’ is better than o as a medoid\nAlgorithm_3 CLARA random sample in huge data do PAM PAM search the whole graph\nCLARA search some random sub-graphs\nAlgorithm_4 Hierarchical Clustering AGNES (agglomerative) Initially, each object is a cluster Step-by-step cluster merging, until all objects form a cluster DIANA (divisive) Initially, all objects are in one cluster Step-by-step splitting clusters until each cluster contains only one object More Details Single-Link（最短距离链接）： Single-Link聚类方法将两个簇之间的距离定义为它们中最接近的两个点之间的距离。换句话说，它测量了两个簇中最相似的成员之间的距离。这种方法倾向于形成具有长而窄的簇，因为它强调了局部相似性。Single-Link在处理非凸形状的簇时表现良好，但容易受到噪声和异常值的影响。\nComplete-Link（最长距离链接）： Complete-Link聚类方法将两个簇之间的距离定义为它们中最不相似的两个点之间的距离。它关注的是两个簇中最不相似的成员之间的距离。这种方法更倾向于形成具有更紧凑形状的簇，因为它强调了簇的全局相似性。Complete-Link对于处理不同大小和不同密度的簇效果较好，但可能会受到异常值的干扰。\nAlgorithm_5 BIRCH Clustering Feature: CF = (N, LS, SS)\nN: #data points\nLS: sum of position\nSS: sum of the square of the position\nPhase 1: scan DB to build an initial inmemory CF tree (a multi-level compression of the data that tries to preserve the inherent clustering structure of the data) Phase 2: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree 一开始只有一个空的root，然后一个一个加进去，如果在已有节点的球体内，则合为一个cluster，如果一个cluster超过一定数量，则分裂：选择最远两个点最为两个新的cluster中心点，重新分配，保证树结构完整性。\nAlgorithm_6 Distance-based Methods Previous Knowledge Eps: Maximum radius of the neighborhood MinPts: Minimum number of points in an Eps- neighborhood of that point NEps(p): {q | dist(p,q) $\\leq$ Eps} Core object p: |Neps(p)| $\\ge$ MinPts Point q directly density-reachable from p iff q $\\in$ Neps(p) and p is a core object Density-reachable: p1 $\\rightarrow$ p2, p2 $\\rightarrow$ p3, \u0026hellip;, pn-1 $\\rightarrow$ pn then pn is density-reachable from p1 Density-connected: Points p, q are density-reachable from o $\\rightarrow$ p and q are density-connected DBSCAN Arbitrary select a point p Retrieve all points directly density-reachable from p wrt Eps and MinPts If p is a core point, a cluster is formed If p is a border point, no points are density- reachable from p and DBSCAN visits the next point of the database Continue the process until all of the points have been processed P.S. 一个一个判断，如果是cluster，且范围内点已形成cluster，则加入，若无先前cluster，则新建立一个。若是后建立的和之前建立的通过一个点reachable了，则将两个cluster合为一个。\n","date":"2023-10-26T00:00:00Z","image":"https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../cluster_hu1f07f723625917ab4cca27f0b859a725_368923_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../","title":"Cluster review notes (updating...)"},{"content":" Employed warp-level primitives to optimize warp reduce of Softmax Kernel, which improved efficiency by 7% Implemented matrix multiplication kernel with matrix chunking and shared memory in C and CUDA Optimized shared memory access via vector read instruction LDS.128, resulting in a 3% efficiency improvement Github Repo Link Docs Link ","date":"2022-12-25T00:00:00Z","image":"https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/GPU_hu335b1d50c3a8ce5c3c8e0318271e4bf3_147350_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/","title":"PyTorch CUDA Operators Parallelism Implementation and Optimization"},{"content":"","date":"2021-12-15T00:00:00Z","image":"https://yiwenlong2001.github.io/p/proof-of-uniqueness-of-the-entropy-formula/entropy_hu67f5226b2bc7a35d1cc75de0da216e33_7981_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/proof-of-uniqueness-of-the-entropy-formula/","title":"Proof of uniqueness of the entropy formula"}]