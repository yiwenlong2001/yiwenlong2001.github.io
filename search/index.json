[{"content":"Education Ungraduate School Shanghai Jiao Tong University Degree：Bachelor Major：Computer Science Date：2019-9-1 to 2023-6-30 Award：2019, 2020, 2021 SJTU Honor Award Award：Outstanding Graduates Relevant coursework Data Science, Artificial Intelligence, Data Structure, Algorithm, Database, Machine Learning, Computer Network, Software Engineer Graduate School University of California, Los Angeles Degree：Master Major：Computer Science Date：2023-9-1 to 2025-5-1 Relevant coursework Reinforcement Learning, Big Data Analysis, Automatic Reasoning Activity SJTU Deputy Director of the Social Practice Department of the Youth League Committee\nUCLA Waiting to be filled\n","date":"2023-09-01T00:00:00Z","image":"https://yiwenlong2001.github.io/p/education/ucla_hueb66c70f98b40f4c1056d1dc09470407_175613_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/education/","title":"Education"},{"content":"Classification Decision Tree 决策树最主要的就是构建决策树，构建决策树有以下几种著名的算法。 ID3算法 ID3算法是一种基于信息熵的算法。通过信息增益找到可以最合适的划分属性。\nGain(S, A) = E(S) - E(S|A)\nE(S|A) = - sum(|S_i|/|S| * E(S_1))\n计算所有的属性对应的信息增益，选择信息增益最大的属性对数据集进行划分。 递归地对之后的数据集进行划分直到： 只剩一个属性无法继续划分 所有的属性信息增益都很小 缺点：\n无剪枝策略，容易过拟合； 只能用于处理离散分布的特征； 没有考虑缺失值。 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1，而信息增益比指数可以解决此缺点。 为了解决这些问题，提出这些改进：\nGain ratio $ SplitInformation = \\sum_{i=1}^{c} \\frac{|S_i|}{|S|} log_2 \\frac{|S_i|}{|S|} $ $ GainRatio(S, A) = \\frac{Gain(S, A)}{SplitInformatio(S, A)} $\nGini Index 如果一个数据集有n各种类的数据，那么gini index被定义为 $$ gini(T) = 1 - \\sum_{j=1}^n p_j^2$$ 其中$ p_j $是种类j再数据集中的频繁程度\n假如我们将数据集T分为大小分别为N1和N2的两个子集T1和T2， 那么此次分割中： $$ gini_{split}(T) = \\frac{N_1}{N} gini(T_1) + \\frac{N_2}{N} gini(T_2) $$\n在各种分割中，选择$ gini_{split}(T) $最小的分法。\n过拟合 针对过拟合可以采用以下两种方法：\nPrepruning: Halt tree construction early—do not split a node if this would result in the goodness measure falling below a threshold Postpruning: Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees 确定树的大小：使用MDL原则\n贝叶斯网络 贝叶斯定理：\ngiven training data x, posteriori probability of a hypothesis H, P(H|X) is $ P(H|X) = \\frac{P(X|H) * P(H)}{P(X)} $\nP(H|X) Posterior probability is the updates probability after the evidence is considered. P(X|H) Likelihood probability naive bayesian classifier \u0026ldquo;naive\u0026rdquo; means it assumes class conditional independence.\nExample:\nP(buy = yes | age \u0026lt;= 30, income = high, student = no).\nApply Bayes\u0026rsquo;s. theorem:\nP(B=Y|A\u0026lt;=30, I=H, S=N) = P(B=Y) * P(A\u0026lt;=30, I=H, S=N | B=Y) / P(A\u0026lt;=30, I=H, S=N) 其中P(B=Y)直接从数据中得到 P(A\u0026lt;=30, I=H, S=N | B=Y)由于独立性可以分解为：P(A\u0026lt;=30 | B=Y) * P(I=H | B=Y) * P(S=N | B=Y)，这三个可以直接从数据中count得到 Bayesian Network No longer independent!\nconsider: Causal relation (depending relationship) instead.\nConditional Independent:\nS and R is not independent. S given C and R given C is independent! for a struction like that:\nC\nS R\nW\nwe can construct P(C, S, R, W) = P(W|S, R) * P(R|C) * P(S|C) * P(C)\nInference: Bottom up.\nFirst we calculate P(W=T) $$ \\sum_{c,s,r} P(C=c, S=s, R=r, W=T)$$ and for two possible reason S and R P(S|R) = P(S, R) / P(R)\nInference: Top down The probability that W=T given that C = T\n","date":"2023-10-30T00:00:00Z","image":"https://yiwenlong2001.github.io/p/classification-review-notes-updating.../classification_hu4fc43e2b18ace3d44063bbef11699032_4250_120x120_fill_box_smart1_3.png","permalink":"https://yiwenlong2001.github.io/p/classification-review-notes-updating.../","title":"Classification review notes (updating...)"},{"content":"NNF 和 对应拓展性质 NNF 输入为variable以及variable的否定，由与或门组成的逻辑网络 这是一种最常见的逻辑网络，我们没办法做什么 DNNF 在NNF的基础上，要求每一个and节点的任意两个分支所涉及的节点交集为空（no overlap） 在这个网络上，NNF的可行性可以被简单地验证 我们可以在多项式时间内执行：CO, CE, ME这三个任务 任务汇总：\nCO: consistency\nVA: Validity\nSE: sentential entailment\nCE: clausal entailment(KB implies clause)\nIP: implicant testing(term implies KB)\nEQ: equivalence testing\nCT: model countig\nME: model enumeration\nDNNF能够在线性时间内完成clause entailment 如果一个NNF子图能够在线性时间内完成clause entailment，我们称之为tractable。DNNF是tractable的。\ndNNF 在NNF的基础上，要求每一个or节点的任意两个分支的输入之间互斥 E.g. $(\\neg A \\land B) \\lor (A \\land \\neg B)$ 就是互斥的\n这意味着无论你怎么取值，or节点的不可能有两个输入同时为true dNNF不是tractable的 sNNF \u0026ldquo;smoothness\u0026rdquo;，在NNF的基础上，要求每一个or节点的任意两个分支所涉及的参数必须相同。 sNNF不是tractable的。 d-DNNF 顾名思义，同时满足dNNF和DNNF。 d-DNNF是tractable的。 d-DNNF可以在多项式时间内实现VA， IP， CT任务。 sd-DNNF 同时满足dNNF和DNNF和sNNF tractable。 f-NNF flatness or shallow circuit 满足性质：网络仅仅只有两层 Simple conjunction：由or-and-variable组成，且每个and下的variable均各不相同，我们可以将这种网络改写为DNF形式。 Simple conjunction同时满足decomposity性质。 Simple disjunction，CNF，同上。 f-NNF和CNF不是tractable。但是DNF是tractable的。 PI prime Implicates, 是一种特殊的CNF，要求CNF满足： 没有一个clause是其他clause的子集（都是有用的clause） 如果可以进行resolution，那么resolution的结果一定本来就可以被其中某个clause imply。 DNF的版本称之为IP PI来自于CNF， 可以在多项式时间内解决CO, CE, ME, VA, IP, SE, EQ问题 IP来自于DNF，可以额外在多项式时间内解决VA, IP, SE, EQ问题。 BDD decision graph： variable的取值由其parent决定。 同时还是decomposition。 FBDD test one property：从任何一条根节点到叶子节点的路径上，每一个variable都只能出现一次 是在d-DNNF和BDD的基础上的，因此他可以在多项式时间内解决七个问题。 OBDD 在任意从根节点到叶子节点的路径上，都满足同一variable的顺序 decision decomposability order 可以在多项式时间内完成SE， EQ任务，因此它在多项式时间内可以完成所有的任务。 DNNF详解 transformation operation:\nCD: conditioning.\nSFO: single variable.\nFO: multiple variable.\n\u0026amp;: conjunction.\nB\u0026amp;: bounded conjoin.\n|: disjoin.\nB|: bounded disjoin.\n~: negate\nSentrntial decision diagram OBDD is influential. SDD: Branch on sentences.\np1, p2, p3 are called primes.\ns1, s2, s3 are called subs.\n","date":"2023-10-28T00:00:00Z","image":"https://yiwenlong2001.github.io/p/nnf-review-notes-updating.../NNF_hu14edb053461a8e06e4f1fdca463dbb61_2574_120x120_fill_box_smart1_3.png","permalink":"https://yiwenlong2001.github.io/p/nnf-review-notes-updating.../","title":"NNF review notes (updating...)"},{"content":"Cluster Algorithm_1 K-means randomly choose initial cluster centroics. assign each point to its nearset centroic. do iteration until the location of any centroics does not change: caluculate the mean value for every point of each cluster. relocate every point. Algorithm_2 PAM Arbitrarily choose k objects as the initial medoids Until no change, do (Re)assign each object to the cluster to which the nearest medoid Randomly select a non-medoid object o’, compute the total cost, S, of swapping medoid o with o’ If S \u0026lt; 0 then swap o with o’ to form the new set of k medoids Swapping Cost: Measure whether o’ is better than o as a medoid\nAlgorithm_3 CLARA random sample in huge data do PAM PAM search the whole graph\nCLARA search some random sub-graphs\nAlgorithm_4 Hierarchical Clustering AGNES (agglomerative) Initially, each object is a cluster Step-by-step cluster merging, until all objects form a cluster DIANA (divisive) Initially, all objects are in one cluster Step-by-step splitting clusters until each cluster contains only one object More Details Single-Link（最短距离链接）： Single-Link聚类方法将两个簇之间的距离定义为它们中最接近的两个点之间的距离。换句话说，它测量了两个簇中最相似的成员之间的距离。这种方法倾向于形成具有长而窄的簇，因为它强调了局部相似性。Single-Link在处理非凸形状的簇时表现良好，但容易受到噪声和异常值的影响。\nComplete-Link（最长距离链接）： Complete-Link聚类方法将两个簇之间的距离定义为它们中最不相似的两个点之间的距离。它关注的是两个簇中最不相似的成员之间的距离。这种方法更倾向于形成具有更紧凑形状的簇，因为它强调了簇的全局相似性。Complete-Link对于处理不同大小和不同密度的簇效果较好，但可能会受到异常值的干扰。\nAlgorithm_5 BIRCH Clustering Feature: CF = (N, LS, SS)\nN: #data points\nLS: sum of position\nSS: sum of the square of the position\nPhase 1: scan DB to build an initial inmemory CF tree (a multi-level compression of the data that tries to preserve the inherent clustering structure of the data) Phase 2: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree 一开始只有一个空的root，然后一个一个加进去，如果在已有节点的球体内，则合为一个cluster，如果一个cluster超过一定数量，则分裂：选择最远两个点最为两个新的cluster中心点，重新分配，保证树结构完整性。\nAlgorithm_6 Distance-based Methods Previous Knowledge Eps: Maximum radius of the neighborhood MinPts: Minimum number of points in an Eps- neighborhood of that point NEps(p): {q | dist(p,q) $\\leq$ Eps} Core object p: |Neps(p)| $\\ge$ MinPts Point q directly density-reachable from p iff q $\\in$ Neps(p) and p is a core object Density-reachable: p1 $\\rightarrow$ p2, p2 $\\rightarrow$ p3, \u0026hellip;, pn-1 $\\rightarrow$ pn then pn is density-reachable from p1 Density-connected: Points p, q are density-reachable from o $\\rightarrow$ p and q are density-connected DBSCAN Arbitrary select a point p Retrieve all points directly density-reachable from p wrt Eps and MinPts If p is a core point, a cluster is formed If p is a border point, no points are density- reachable from p and DBSCAN visits the next point of the database Continue the process until all of the points have been processed P.S. 一个一个判断，如果是cluster，且范围内点已形成cluster，则加入，若无先前cluster，则新建立一个。若是后建立的和之前建立的通过一个点reachable了，则将两个cluster合为一个。\n","date":"2023-10-26T00:00:00Z","image":"https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../cluster_hu1f07f723625917ab4cca27f0b859a725_368923_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/cluster-review-notes-updating.../","title":"Cluster review notes (updating...)"},{"content":" Employed warp-level primitives to optimize warp reduce of Softmax Kernel, which improved efficiency by 7% Implemented matrix multiplication kernel with matrix chunking and shared memory in C and CUDA Optimized shared memory access via vector read instruction LDS.128, resulting in a 3% efficiency improvement Github Repo Link Docs Link ","date":"2022-12-25T00:00:00Z","image":"https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/GPU_hu335b1d50c3a8ce5c3c8e0318271e4bf3_147350_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/pytorch-cuda-operators-parallelism-implementation-and-optimization/","title":"PyTorch CUDA Operators Parallelism Implementation and Optimization"},{"content":"","date":"2021-12-15T00:00:00Z","image":"https://yiwenlong2001.github.io/p/proof-of-uniqueness-of-the-entropy-formula/entropy_hu67f5226b2bc7a35d1cc75de0da216e33_7981_120x120_fill_q75_box_smart1.jpg","permalink":"https://yiwenlong2001.github.io/p/proof-of-uniqueness-of-the-entropy-formula/","title":"Proof of uniqueness of the entropy formula"},{"content":"We will start from SDD. And previous content will be posted later.\nSDD SDD is a kind of OBDD, which means the order of variable is needed in such circuit. And SDD is unique when canonical / reduced.\nStructured Decomposability Decomposability:\nf(ABCD) = $(g_1(AB) \\land h_1(CD)) \\lor (g_2(A) \\land h2(BCD)) \\lor \u0026hellip;$\nStructured Decomposability:\nf(ABCD) = $(g_1(AB) \\land h_1(CD)) \\lor (g_2(AB) \\land h2(CD)) \\lor \u0026hellip;$\nThe difference between these two kinds of decomposability is whether variables split in the same way in each subfunction.\nDefinition of Structured Decomposability:\nA (X,Y)-partition of a function f goes like: $$f(X, Y) = g_1(X)h_1(Y) + \u0026hellip; + g_n(X)h_n(Y)$$ Where $X \\land Y = \\empty$ and $X \\lor Y$ = all variables in f.\n$g_i$ regarding X is called a prime and $h_i$ regarding Y is called a sub, which requires that:\n$$ \\forall i, j, \\ g_i \\land g_j = False $$ $$g_1 \\lor g_2 \u0026hellip;\\lor g_i = True$$ $$\\forall i, \\ g_i \\neq False $$\nA (X,Y)-partition is compressed if there is no equal subs. That is, $$h_i \\neq h_j\\ \\ \\ \\forall i \\neq j$$\nVtree Vtree is a binary tree that denotes the order and the structure of a SDD. Each node’s left branch refers to the element in the primes, and each node’s right branch refers to that of the subs.\nOBDD is a special case of SDD with right-linear a vtree, whose left branch is always a leaf.\nSDD is a strict superset of OBDD, maintaining key properties of OBDD b, and could be exponentially smaller than OBDD.\nBuilding a SDD For example, we have $f = (A \\land B) \\lor (B \\land C) \\lor (C \\land D)$ and X = {A, B}, Y = {C, D}.\nThen we can have the sub-functions (subs) as condi- tioned on the primes:\nprime sub $A \\land B$ True $A \\land \\neg B$ $C \\land D$ $\\neg A \\land B$ C $\\neg A \\land \\neg B$ $C \\land D$ Resolving the primes with the same sub, to conduct compression:\nprime sub $A \\land B$ True $\\neg B$ $C \\land D$ $\\neg A \\land B$ C Then we could have f = ($A \\land B$)(True) + ($\\neg A \\land B$)(C) + ($\\neg B$)($C \\land D$)\nFor Vtree, if X and Y are fixed, the leaves under the left branch of the root has to contain and only contain variables belong to X, and right branch for Y. For intermediate nodes (neither leave nor root), do the same recursively.\nThen we can construct the SDD according to the Vtree and f.\nEach node consists of a head and a tail; for either a head or a tail, if it involves more than one variable (a.k.a representing an intermediate node in the vtree), we need to decompose it again (according to its left-right branches in the vtree).\nMost importantly: I have made this mistake in my Homework. After you constructing the SDD, you need to check whether this SDD can be reduced, that is, whether there are some node that are just the same. You need to combine these nodes.\nPolytiom Operation if you have two (X,Y)-partition like: $$f: (p_1, q_1) \u0026hellip; (p_n, q_n)$$ $$g: (r_1, s_1) \u0026hellip; (r_m, s_m)$$ Then for any logic operator \u0026amp;, we have: $$f\\ \u0026amp; \\ g : (p_i \\land r_j, q_i\\ \u0026amp; \\ s_j), \\forall i, j, p_i \\land r_j \\neq False$$\nBottom-Up Compliation for OBDD/SDD If we want to compile a CNF:\nFirst, we comstruct OBDD/SDD for literals.\nThen, combine literals into clause.\nFinally, combine all the literals into CNF.\nAlso works for DNF.\nCanonicity in Compilation fixed variable order -\u0026gt; unique reduced OBDD fixed vtree -\u0026gt; unique trimmed \u0026amp; compressed SDD\nHowever, if we have n variable, then we can have n! kinds of variable order. Then we also have $C_{n-1}$ kinds of dissections(prime and sub). Then we will have: $$n!\\ *\\ C_{n-1} = \\frac{(2(n-1))!}{(n-1)!}$$ kinds of vtree in total.\nOperation on vtree Tree rotate. Tree swap. Search over vtree\nWe would use greedy search to enumerate all the vtrees. First, we enumerate all vtree over a window, that is, all the vtrees that can be reachable via a certain amount of rotate/swap operations. Then we greedily accept the best vtree that has been found, and then move window.\nPSDD probability space -\u0026gt; the truth table -\u0026gt; (world, instantiation, 1/0)\nSDD -\u0026gt; Tractable Boolean Circuit\nPSDD -\u0026gt; probabilistic -\u0026gt; (world, instantiation, Pr())\nFrom SDD to PSDD PSDD, compared to SDD, is almost the same, except that:\nOR-gates: having probability distributions over all inputs.\nAny two OR-gates may have di↵erent probability distributions.\nThe AND-gates are just kept the same and no pro- bability applies.\nCalculating rule If we have an input like L, K, P, A, and we need to calculate the probability of this world. We can do following sequencely.\nFirstly, we check bottom up. Find all leaves that statisfy the input. For or gate, we continue if one of its branch is selected, and for and gate, we continue only if all its branchs are selected.\nThen, we calculate top down. From the top gate, for or gate, we add all its selected branch, and for and gate, we multiply all its branches.\nFor example, in the above figure, we can have: $$Pr(L, K, P, A) = 0.3 * 1.0 * 0.8 * 0.4 * 0.25$$\nIf our input does not contain all the literals, for example: L, P, A. Then we still could follow the above-mentioned steps and we can have: $$Pr(L, P, A) = 0.3 * 1.0 * 1.0 * 0.4 * 0.25$$\nPrime Implicate and Prime Implicant Firstly, in this section, we denote $\\Delta$ as the knowledge base.\nMost importantly, we denote prime Implicate (CNF) as PI and prime implicant (DNF) as IP.\nPI: CNF that contains no subsumed clauses.\nIP: DNF that contains no subsumed terms.\nTo obtain PI: close $\\Delta$ under resolution then drop subsumed clauses.\nTo obtain IP: close $\\Delta$ under consensus then drop subsumed terms.\nP.S. remember! When you are doing resolution/consensus, you can only invlove one literal, that is, you can not do resolution on (A, B, C) and (A, $\\neg B$, $\\neg C$).\nSubsume: all-literals already contained.\nClause: c1 subsumes c2 -\u0026gt; c1 is shorter than c2 -\u0026gt; c1 entails c2.\nterms: t1 subsumes t2 -\u0026gt; t1 is shorter than t2 -\u0026gt; t2 entail t1.\nDuality: $\\alpha$ is PI of $\\Delta$, then $\\neg \\alpha$ is IP of $\\neg \\Delta$.\nModel-based Diagnosis Model-based Diagnosis can be divided into several parts.\nFirstly, we need to construct the system. For a given circuit, we will have several logic variables and several health variables. Each logic gate has a logic equation, that is, $OK_i \\rightarrow $ logic equation of this gate.\nWe define the system observation $\\alpha$ and then construct the model-based diagnosis as $\\Delta \\land \\alpha \\land OK_{situation}$. $OK_{situation}$ denotes the situation we need to diagnosis. For example, we suppose that OK1 and OK2 are both health, then we need to test $\\Delta \\land \\alpha \\land OK_1 \\land OK_2$ is statisfiable. If so, then we can say that situation is normal, otherwise it is abnormal.\nTo do model-based diagnosis, we conclude all the normal assignment of the health variables and sinplify.\nHealth condition Health condition of system $\\Delta$ given observation $\\alpha$ is:\n$$Health(\\Delta, \\alpha) = \\exists\\ (all\\ variables\\ except\\ health\\ variables)\\ \\Delta \\land \\alpha$$ That is, projection of $\\Delta \\land \\alpha$ onto health variables.\nBased on health condition Health($\\Delta$,$\\alpha$) we can do model-based diagnosis:\nConflict: implicates of Health($\\Delta$,$\\alpha$). Min-conflict: PI of Health($\\Delta$,$\\alpha$). Parlid: implicant of Health($\\Delta$,$\\alpha$). Kernel: IP of Health($\\Delta$,$\\alpha$). Classifier Function version of a classifier: $$f(x_1, x_2, \u0026hellip;, x_n)$$\n$x_i$ are called features. all features $x_1,x_2,\u0026hellip;x_n$ together: instance. output of f: decision (classifica- tion); positive/negative decision refer to f = 1/0 respectively, while the corresponding instances are called positive/negative instantiation. Boolean Classifier: $x_i$, f have Boolean values.\nMonotone Classifier: positive instance remains positive if we flip some features from negative to positive.\nExplainable AI Universal Literal Quantification.\nFor binary variable: $$\\forall x . \\Delta = \\Delta|x \\land (x \\lor \\Delta|\\neg x) $$ For discrete values variables: $$\\forall x_i . \\Delta = \\Delta|x_i \\land \\land_{j\\neq i}(x_i \\lor \\Delta|x_j)$$\nGeneral Universal Literal Quantification.\n$$\\hat{\\forall} x_i . \\Delta = \\Delta|x_i \\land \\Delta $$ Alternatively, $$\\hat{\\forall} x_i . \\Delta = \\Delta|x_i \\land \\land_{j\\neq i}(\\neg x_j \\lor \\Delta|x_j)$$\nNow we can introduce the most important definitions in explainable AI.\n$\\Delta$ is the class formula. $I$ is the instance. $\\forall I . \\Delta$ is complete reason. The PI of complete reason is Sufficient reasons. The IP of complete reason is Necessary reasons. $\\hat{\\forall} I . \\Delta$ is general complete reason. The PI of general complete reason is Genral Sufficient reasons. The IP of general complete reason is General Necessary reasons. Discrete Logic ","date":"0001-01-01T00:00:00Z","image":"https://yiwenlong2001.github.io/p/cs264-review-note-updating.../NNF_hu14edb053461a8e06e4f1fdca463dbb61_2574_120x120_fill_box_smart1_3.png","permalink":"https://yiwenlong2001.github.io/p/cs264-review-note-updating.../","title":"CS264 review note (updating...)"}]